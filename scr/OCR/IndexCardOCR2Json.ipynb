{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index Card OCR Workflow\n",
    "gestestet in Python 3.9.18 Venv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labelingtool für die Basisannotation\n",
    "- https://pypi.org/project/labelImg/ \n",
    "- funktioniert bestens, Output in YOLO, XML, CreateML (JSON)\n",
    "- für den folgenden Workflow wird der CreateML-Export geparst\n",
    "- Bounding Boxes wurden alle großzügig gezogen, der Labelname mit einbezogen und als Label vergeben. So kann später das Label aus dem OCR-Ergebnis gelöscht werden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install labelImg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!labelImg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basisworkflow OCR "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation Apple-Vision-OCR-Wrapper für Python + OpenCV-Python-Wrapper + Fuzzy String-Vergleich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install Torch NumPy Pandas Pillow scikit-learn Plotly pyobjc-framework-Vision\n",
    "!pip install apple_ocr\n",
    "!pip install opencv-python\n",
    "!pip install rapidfuzz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wie es funktioniert\n",
    "1. Laden eines Bildes, ausrichten anhand eines Template-Bildes aus dem gleichen Bestand, dass auch für das Bounding-Box-Labeling benutzt wurde. \n",
    "2. OCR \n",
    "3. Erstellen einer Liste aller Bounding-Boxes aus der OCR mit ihren Werten (Inhalten)\n",
    "4. Erstellen einer Liste aller Bounding-Boxes aus dem Labeling mit ihren Labeln \n",
    "5. Serieller Vergleich aller Labelboxen mit allen OCR-Boxen. Liegt eine OCR-Box zu 90% innerhalb einer Labelbox (Grenzwert kann justiert werden), wird der Wert für diese Labelbox registriert\n",
    "6. Bereinigung des Ergebnisses um alle Label-Begriffe. Dafür werden die OCR-Resultate in Einzelworte zerlegt, mit dem Label fuzzy abgeglichen und anschließend wieder kombiniert. Bereinigung derzeit ebenfalls um Anführungszeichen, da diese im JSON Probleme machen können.\n",
    "7. Konstruktion eines JSON mit dem Dateinamen als ID, Zuordnung der gematchten OCR-Boxen zu den Labeln \n",
    "8. Ausgabe\n",
    "\n",
    "#### Pfade\n",
    "Insgesamt werden am Ende des Scripts drei Pfade benötigt: \n",
    "- `folder` – der Ordnerpfad zu den zu scannenden Bildern\n",
    "- `label_template_path`– das Template mit den Labelboxen (ein JSON-File)\n",
    "- `align_template_path` – das Bild, an dem alle vor der OCR ausgerichtet werden sollen, idealerweise das Bild für das Label-Template.\n",
    "\n",
    "#### Voransicht\n",
    "Im Script ist eine `image.show()` Anweisung, die eine Voransicht der Bounding-Boxes erzeugt, die Label sind rot, die OCR-Resultate blau. So kann geprüft werden, ob die gewünschten Elemente innerhalb der Labelboxen liegen. Die Voransicht verlängert den Umwandlungsprozess und kann auskommentiert werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo der OCR\n",
    "Ausgabe erfolgt als Liste mit einer Eckkoordinate, einer Box-Länge und einem Mittelpunkt, daraus kann die zweite Eckkoordinate abgeleitet werden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  Content    Length   Density         x  \\\n",
      "0            Komponist: Scholz, Siegfried  0.414634  0.024337  0.108749   \n",
      "1                                  Titel:  0.052326  0.001589  0.107558   \n",
      "2                   Signatur: Spez•20•512  0.300872  0.012213  0.636628   \n",
      "3                    Stille kleine Straße  0.348837  0.014263  0.228198   \n",
      "4                                 Foxtrot  0.125000  0.005058  0.229167   \n",
      "5                 Orch • Walter Kubiczeck  0.380814  0.014693  0.508721   \n",
      "6                   Ges • : Giso Weisbach  0.331395  0.012095  0.507267   \n",
      "7                Textdichter: Helga Heine  0.332849  0.014851  0.106105   \n",
      "8                                 Verlag:  0.075581  0.002448  0.107558   \n",
      "9  Material: 1 Part •u•Stim• Bemerkungen:  0.514535  0.023956  0.109012   \n",
      "\n",
      "          y  Centroid x  Centroid y  \n",
      "0  0.763285    0.316066    0.792632  \n",
      "1  0.714575    0.133721    0.729757  \n",
      "2  0.783294    0.787064    0.803591  \n",
      "3  0.601051    0.402616    0.621495  \n",
      "4  0.526012    0.291667    0.546243  \n",
      "5  0.473684    0.699128    0.492975  \n",
      "6  0.435164    0.672965    0.453412  \n",
      "7  0.346154    0.272529    0.368462  \n",
      "8  0.267206    0.145349    0.283401  \n",
      "9  0.190283    0.366279    0.213563  \n"
     ]
    }
   ],
   "source": [
    "from apple_ocr.ocr import OCR\n",
    "from PIL import Image\n",
    "\n",
    "image_path = \"00001a_Spez_Komp_1_Spez.20.512.jpg\"\n",
    "image = Image.open(image_path)\n",
    "ocr_instance = OCR(image=image)\n",
    "dataframe = ocr_instance.recognize()\n",
    "df = pd.DataFrame(dataframe)\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apple_ocr.ocr import OCR\n",
    "from PIL import Image, ImageDraw\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from rapidfuzz import fuzz\n",
    "import cv2\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "class LoadOCR:\n",
    "    def __init__(self, image_path):\n",
    "        self.image_path = image_path\n",
    "        self.image = cv2.imread(self.image_path)\n",
    "        if self.image is None:\n",
    "            raise ValueError(f\"Could not load image from {image_path}\")\n",
    "\n",
    "    def cv2_align(self, template_path):\n",
    "        template = cv2.imread(template_path)\n",
    "        if template is None:\n",
    "            raise ValueError(f\"Could not load template from {template_path}\")\n",
    "        \n",
    "        scan = self.image\n",
    "        \n",
    "        template_gray = cv2.cvtColor(template, cv2.COLOR_BGR2GRAY)\n",
    "        scan_gray = cv2.cvtColor(scan, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        def detect_text_features(img):\n",
    "            sift = cv2.SIFT_create()\n",
    "            keypoints, descriptors = sift.detectAndCompute(img, None)\n",
    "            return keypoints, descriptors\n",
    "\n",
    "        template_kp, template_desc = detect_text_features(template_gray)\n",
    "        scan_kp, scan_desc = detect_text_features(scan_gray)\n",
    "        \n",
    "        if template_desc is None or scan_desc is None:\n",
    "            raise ValueError(\"No features detected in one or both images\")\n",
    "        \n",
    "        bf = cv2.BFMatcher()\n",
    "        matches = bf.knnMatch(template_desc, scan_desc, k=2)\n",
    "        \n",
    "        good_matches = []\n",
    "        for m, n in matches:\n",
    "            if m.distance < 0.75 * n.distance:\n",
    "                good_matches.append(m)\n",
    "        \n",
    "        if len(good_matches) < 4:\n",
    "            raise ValueError(\"Not enough good matches found for alignment\")\n",
    "        \n",
    "        template_pts = np.float32([template_kp[m.queryIdx].pt for m in good_matches])\n",
    "        scan_pts = np.float32([scan_kp[m.trainIdx].pt for m in good_matches])\n",
    "        \n",
    "        H, mask = cv2.findHomography(scan_pts, template_pts, cv2.RANSAC, 5.0)\n",
    "        \n",
    "        if H is None:\n",
    "            raise ValueError(\"Could not compute homography\")\n",
    "\n",
    "        aligned_scan = cv2.warpPerspective(scan, H, (template.shape[1], template.shape[0]))\n",
    "        \n",
    "        return aligned_scan\n",
    "\n",
    "    def OCR_process(self, template_path):\n",
    "        aligned_image = self.cv2_align(template_path)\n",
    "        aligned_rgb = cv2.cvtColor(aligned_image, cv2.COLOR_BGR2RGB)\n",
    "        aligned_pil = Image.fromarray(aligned_rgb)\n",
    "\n",
    "        ocr_instance = OCR(image=aligned_pil)\n",
    "        dataframe = ocr_instance.recognize()\n",
    "        self.df = pd.DataFrame(dataframe)\n",
    "        self.image = aligned_pil\n",
    "        return self.df, self.image\n",
    "    \n",
    "class BoundingBox:\n",
    "  def __init__(self, x_min, y_min, x_max, y_max, value):\n",
    "      self.x_min = x_min\n",
    "      self.y_min = y_min\n",
    "      self.x_max = x_max\n",
    "      self.y_max = y_max\n",
    "      self.value = value\n",
    "\n",
    "class BoundingBoxCollection:\n",
    "  def __init__(self):\n",
    "        self.bboxes = []\n",
    "\n",
    "  def add_bbox(self, x_min, y_min, x_max, y_max, value):\n",
    "      bbox = BoundingBox(x_min, y_min, x_max, y_max, value)\n",
    "      self.bboxes.append(bbox)\n",
    "\n",
    "  def process_boxes(self, df):\n",
    "          for index, row in df.iterrows():\n",
    "              norm_x = row['x']\n",
    "              norm_y = row['y']\n",
    "              centroid_x = row['Centroid x']\n",
    "              centroid_y = row['Centroid y']\n",
    "              density = row['Density']  \n",
    "              length = row['Length']\n",
    "\n",
    "              norm_x1 = row['x']\n",
    "              norm_y2 = 1 - row['y']\n",
    "              norm_x2 = norm_x1 + length\n",
    "              norm_y1 = 1 - norm_y - (abs(centroid_y - norm_y) * 2)  \n",
    "              self.add_bbox(norm_x1, norm_y1, norm_x2, norm_y2, row['Content'])\n",
    "              \n",
    "  def draw_boxes(self, image, draw):\n",
    "      image_width, image_height = image.size\n",
    "      for bbox in self.bboxes:\n",
    "\n",
    "          actual_x = int(bbox.x_min * image_width)\n",
    "          actual_y = int((bbox.y_max) * image_height)  \n",
    "          \n",
    "          box_width = int((bbox.x_max - bbox.x_min) * image_width)\n",
    "          box_height = int((bbox.y_max - bbox.y_min) * image_height)\n",
    "          \n",
    "          actual_bottom_right_x = actual_x + box_width\n",
    "          actual_top_left_y = actual_y - box_height\n",
    "          \n",
    "          draw.rectangle([actual_x, actual_top_left_y, actual_bottom_right_x, actual_y], \n",
    "                      outline=\"blue\", width=2)\n",
    "          draw.text((actual_x, actual_y - 10), bbox.value, fill=\"blue\")\n",
    "\n",
    "class ImageAnnotationProcessor:\n",
    "    def __init__(self, json_path):\n",
    "        self.json_path = json_path\n",
    "        self.data = None\n",
    "        self.image = None\n",
    "        self.draw = None\n",
    "        self.image_width = None \n",
    "        self.image_height = None\n",
    "        self.labelbboxes = None\n",
    "\n",
    "    def process_annotations(self, labelbboxes):\n",
    "        self.labelbboxes = labelbboxes\n",
    "        \n",
    "        with open(self.json_path) as f:\n",
    "            self.data = json.load(f)\n",
    "\n",
    "        image_path = \"./images/\" + self.data[0][\"image\"]\n",
    "        self.image = Image.open(image_path)\n",
    "        self.image_width, self.image_height = self.image.size\n",
    "        \n",
    "        for annotation in self.data[0][\"annotations\"]:\n",
    "            label = annotation[\"label\"]\n",
    "            coordinates = annotation[\"coordinates\"]\n",
    "            \n",
    "            x_center = coordinates[\"x\"] / self.image_width  \n",
    "            y_center = coordinates[\"y\"] / self.image_height\n",
    "            width = coordinates[\"width\"] / self.image_width\n",
    "            height = coordinates[\"height\"] / self.image_height\n",
    "\n",
    "            x1 = x_center - width / 2\n",
    "            y1 = y_center - height / 2\n",
    "            x2 = x_center + width / 2\n",
    "            y2 = y_center + height / 2\n",
    "            \n",
    "            self.labelbboxes.add_bbox(x1, y1, x2, y2, label)\n",
    "\n",
    "    def draw_annotations(self, image):\n",
    "        self.draw = ImageDraw.Draw(image)\n",
    "        image_width, image_height = image.size\n",
    "        \n",
    "        for bbox in self.labelbboxes.bboxes:\n",
    "            x1_scaled = bbox.x_min * image_width\n",
    "            y1_scaled = bbox.y_min * image_height\n",
    "            x2_scaled = bbox.x_max * image_width\n",
    "            y2_scaled = bbox.y_max * image_height\n",
    "            \n",
    "            self.draw.rectangle(\n",
    "                [x1_scaled, y1_scaled, x2_scaled, y2_scaled], \n",
    "                outline=\"red\", \n",
    "                width=3\n",
    "            )\n",
    "            self.draw.text(\n",
    "                (x1_scaled, y1_scaled - 10), \n",
    "                bbox.value, \n",
    "                fill=\"red\"\n",
    "            )\n",
    "        \n",
    "        return self.image\n",
    "    \n",
    "class BBoxCompare:\n",
    "    def __init__(self, x_min, y_min, x_max, y_max, value):\n",
    "        self.x_min = x_min\n",
    "        self.y_min = y_min\n",
    "        self.x_max = x_max\n",
    "        self.y_max = y_max\n",
    "        self.value = value\n",
    "\n",
    "    def area(self):\n",
    "        \"\"\"Calculate the area of the bounding box.\"\"\"\n",
    "        return (self.x_max - self.x_min) * (self.y_max - self.y_min)\n",
    "\n",
    "    def intersection(self, other):\n",
    "\n",
    "        x_min_inter = max(self.x_min, other.x_min)\n",
    "        y_min_inter = max(self.y_min, other.y_min)\n",
    "        x_max_inter = min(self.x_max, other.x_max)\n",
    "        y_max_inter = min(self.y_max, other.y_max)\n",
    "\n",
    "        inter_width = max(0, x_max_inter - x_min_inter)\n",
    "        inter_height = max(0, y_max_inter - y_min_inter)\n",
    "\n",
    "        intersection_area = inter_width * inter_height\n",
    "\n",
    "        bbox2_area = other.area()\n",
    "        if bbox2_area == 0: \n",
    "            return False\n",
    "        containment_ratio = intersection_area / bbox2_area\n",
    " \n",
    "        return containment_ratio \n",
    "    \n",
    "    def OCRStrClean(self):\n",
    "            if not fuzz.ratio(bbox1.value,bbox2.value) >= 95:\n",
    "                strelems = bbox2.value.split(\" \")\n",
    "                ocrbboxval = []\n",
    "                for strelem in strelems:\n",
    "                    if not fuzz.partial_ratio(bbox1.value,strelem) >= 85:\n",
    "                        ocrbboxval.append(strelem)\n",
    "                ocrbboxval = \" \".join(ocrbboxval)\n",
    "                if ocrbboxval != \"\":\n",
    "                    return ocrbboxval.replace(\"\\\"\",\"?\").replace(\"\\'\",\"?\")     \n",
    "\n",
    "#folder = \"/Users/admin/Downloads/DE-MUS-905113_Inventarkarten_Muehlhausen_2Lfg_Schub02_master/test\"\n",
    "#label_template_path = './images/card1.json'\n",
    "#align_template_path = \"/Volumes/QSTICK/Sicherung/Uni/SODa/Ideen/OCR/Images/DE-MUS-905113_Inventarkarten_Muehlhausen_2Lfg_Schub02_master/_test/template.jpg\"\n",
    "\n",
    "folder = \"images\"\n",
    "label_template_path = 'label_Eisenach.json'\n",
    "align_template_path = '00001a_Spez_Komp_1_Spez.20.512.jpg'\n",
    "\n",
    "for file_name in os.listdir(folder):\n",
    "    if file_name.endswith(('.png', '.jpg', '.jpeg')):\n",
    "        ocr_loader = LoadOCR(os.path.join(folder, file_name))\n",
    "        df, image = ocr_loader.OCR_process(align_template_path)\n",
    "        #print(df)\n",
    "        \n",
    "        ocrbboxes = BoundingBoxCollection()\n",
    "        ocrbboxes.process_boxes(df)\n",
    "\n",
    "        labelbboxes = BoundingBoxCollection()\n",
    "        processor = ImageAnnotationProcessor(label_template_path)\n",
    "        processor.process_annotations(labelbboxes)\n",
    "\n",
    "        draw = ImageDraw.Draw(image)\n",
    "        ocrbboxes.draw_boxes(image, draw)\n",
    "        #image.save(file_name + '_ocrresults.jpg')\n",
    "        annotated_image = processor.draw_annotations(image)\n",
    "        #annotated_image.save('annotated.jpg')\n",
    "        image.show()  \n",
    "\n",
    "        results = {}\n",
    "        results[\"ID\"] = file_name\n",
    "\n",
    "        for bbox in labelbboxes.bboxes:\n",
    "            bbox1 = BBoxCompare(bbox.x_min,bbox.y_min,bbox.x_max,bbox.y_max,bbox.value)\n",
    "            results[bbox1.value] = []  \n",
    "  \n",
    "            for bbox in ocrbboxes.bboxes:\n",
    "                bbox2 = BBoxCompare(bbox.x_min,bbox.y_min,bbox.x_max,bbox.y_max,bbox.value)\n",
    "\n",
    "                containment_ratio = bbox1.intersection(bbox2)\n",
    "                # make sure that at least for \"Signatur\" ocrbbox can be off quite a bit\n",
    "                if  bbox1.value == \"Signatur\":  \n",
    "                    containment_ratio_val = 0.5\n",
    "                else:\n",
    "                    containment_ratio_val = 0.9\n",
    "\n",
    "                if containment_ratio >= containment_ratio_val:\n",
    "                    cleaned_ocrstr = bbox2.OCRStrClean()\n",
    "                    if cleaned_ocrstr is not None:\n",
    "                        results[bbox1.value].append(cleaned_ocrstr)\n",
    "\n",
    "        print(results),print(',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
